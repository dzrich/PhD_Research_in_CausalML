{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn pyro-ppl causalml econml pgmpy dowhy"
      ],
      "metadata": {
        "id": "Mf2TEuXHJrPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#from causalml.inference.tree import CausalForest\n",
        "from dowhy import CausalModel\n",
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
        "from econml.dml import CausalForestDML\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "import pyro.distributions as dist\n",
        "import pyro\n",
        "import torch"
      ],
      "metadata": {
        "id": "Lp9YCQcxJwsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate some credit risk data\n",
        "np.random.seed(42)\n",
        "n = 1000\n",
        "X = np.random.normal(0, 1, size=(n, 3))  # 3 features: credit score, income, loan amount\n",
        "T = np.random.binomial(1, p=0.5, size=(n,))  # Treatment: 0 (low interest rate), 1 (high interest rate)\n",
        "Y = (X[:, 0] * 0.8 - X[:, 1] * 0.3 + T * 1.5 + np.random.normal(0, 1, size=n)) > 0  # Outcome: default or not\n",
        "data = pd.DataFrame(np.column_stack([X, T, Y]), columns=[\"credit_score\", \"income\", \"loan_amount\", \"interest_rate\", \"default\"])\n"
      ],
      "metadata": {
        "id": "ir1AImflJ5Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.view_model()"
      ],
      "metadata": {
        "id": "3eY6u6_0KHWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SCM using DoWhy and specify the causal graph\n",
        "model = CausalModel(\n",
        "    data=data,\n",
        "    treatment=\"interest_rate\",\n",
        "    outcome=\"default\",\n",
        "    common_causes=[\"credit_score\", \"income\", \"loan_amount\"],\n",
        "    instruments=None  # You can add instruments for IV methods\n",
        ")\n",
        "\n",
        "# Identify causal effect\n",
        "identified_estimand = model.identify_effect()\n",
        "\n",
        "# Estimate the effect using propensity score matching\n",
        "estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.propensity_score_matching\")\n",
        "print(\"Estimated Causal Effect:\", estimate.value)"
      ],
      "metadata": {
        "id": "8Bm6G9-nJ_dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ipmort Important Libraries\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ],
      "metadata": {
        "id": "VqQn8bD8iM1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The raw link to the CSV file in the GitHub repository\n",
        "url = 'https://raw.githubusercontent.com/dzrich/PhD_Research_in_CausalML/main/Data/CreditRiskDataset.csv'\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "CreditRiskDataset = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(CreditRiskDataset.head())"
      ],
      "metadata": {
        "id": "6_r1zn-kiRXF",
        "outputId": "3b5433bd-2692-473e-9915-8c28ab3894ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   person_age  person_income person_home_ownership  person_emp_length  \\\n",
            "0          22          59000                  RENT              123.0   \n",
            "1          21           9600                   OWN                5.0   \n",
            "2          25           9600              MORTGAGE                1.0   \n",
            "3          23          65500                  RENT                4.0   \n",
            "4          24          54400                  RENT                8.0   \n",
            "\n",
            "  loan_intent loan_grade  loan_amnt  loan_int_rate  loan_status  \\\n",
            "0    PERSONAL          D      35000          16.02            1   \n",
            "1   EDUCATION          B       1000          11.14            0   \n",
            "2     MEDICAL          C       5500          12.87            1   \n",
            "3     MEDICAL          C      35000          15.23            1   \n",
            "4     MEDICAL          C      35000          14.27            1   \n",
            "\n",
            "   loan_percent_income cb_person_default_on_file  cb_person_cred_hist_length  \\\n",
            "0                 0.59                         Y                           3   \n",
            "1                 0.10                         N                           2   \n",
            "2                 0.57                         N                           3   \n",
            "3                 0.53                         N                           2   \n",
            "4                 0.55                         Y                           4   \n",
            "\n",
            "     OTHER_DEBT  TIME_WITH_BANK  LOAN_PERIOD in days  Unnamed: 15  Unnamed: 16  \n",
            "0  31681.020000               7                316.0          NaN          NaN  \n",
            "1  31437.140180               6                314.0          NaN          NaN  \n",
            "2  23979.127160               9                239.0          NaN          NaN  \n",
            "3   2602.283057               5                260.0          NaN          NaN  \n",
            "4  59393.203400               8                593.0          NaN          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Engineering\n"
      ],
      "metadata": {
        "id": "hhLZ-FFDAX9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Data Engineering\n",
        "def data_engineering(CreditRiskDataset):\n",
        "    # Example: Data cleaning, feature engineering, handling missing values, etc.\n",
        "    # This step will vary depending on your dataset and use case.\n",
        "    CreditRiskDataset = CreditRiskDataset.dropna()\n",
        "    # Feature selection and engineering\n",
        "    # Example: Assume 'X' are features and 'y' is the target\n",
        "    X = CreditRiskDataset.drop('cb_person_default_on_file', axis=1)\n",
        "    y = CreditRiskDataset['cb_person_default_on_file']\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "zHWJZXnYAS28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Correlational ML Techniques - Training Correlational ML Models\n",
        "def train_corr_ml_model(X_train, y_train):\n",
        "    # Example: Random Forest Classifier\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ],
      "metadata": {
        "id": "2FBdsWAlpLjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dowhy"
      ],
      "metadata": {
        "id": "cB1wYUZUpUle",
        "outputId": "0ec67730-2d26-4d0f-9211-e1fd833c7c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dowhy\n",
            "  Downloading dowhy-0.11.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting causal-learn>=0.1.3.0 (from dowhy)\n",
            "  Downloading causal_learn-0.1.3.8-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.5.3)\n",
            "Requirement already satisfied: cython>=0.29.32 in /usr/local/lib/python3.10/dist-packages (from dowhy) (3.0.11)\n",
            "Requirement already satisfied: joblib>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.4.2)\n",
            "Requirement already satisfied: networkx>=2.8.5 in /usr/local/lib/python3.10/dist-packages (from dowhy) (3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from dowhy) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>1.0 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from dowhy) (0.14.3)\n",
            "Requirement already satisfied: sympy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from dowhy) (1.13.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from dowhy) (4.66.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.7.1)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from causal-learn>=0.1.3.0->dowhy) (3.0.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy) (0.6.7.post0)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy) (2.0.14)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.2.2->dowhy) (3.2.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->dowhy) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>1.0->dowhy) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.5->dowhy) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.5->dowhy) (24.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.10.1->dowhy) (1.3.0)\n",
            "Requirement already satisfied: qdldl in /usr/local/lib/python3.10/dist-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.2.2->dowhy) (0.1.7.post4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.5->dowhy) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->causal-learn>=0.1.3.0->dowhy) (3.1.4)\n",
            "Downloading dowhy-0.11.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading causal_learn-0.1.3.8-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.5/174.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: causal-learn, dowhy\n",
            "Successfully installed causal-learn-0.1.3.8 dowhy-0.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dowhy\n",
        "from dowhy import CausalModel"
      ],
      "metadata": {
        "id": "VYj-sF7TpVcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Causal ML Techniques - Training Causal ML Models\n",
        "def train_causal_ml_model(X_train, y_train, CreditRiskDataset):\n",
        "    # Using DoWhy for causal inference\n",
        "    # Define the causal model\n",
        "    causal_model = CausalModel(\n",
        "        CreditRiskDataset=CreditRiskDataset,\n",
        "        treatment=\"treatment\",  # Placeholder; adjust as needed\n",
        "        outcome=\"outcome\",  # Placeholder; adjust as needed\n",
        "        common_causes=[\"common_cause1\", \"common_cause2\"]  # Placeholder; adjust as needed\n",
        "    )\n",
        "    # Identify causal effect\n",
        "    identified_estimand = causal_model.identify_effect()\n",
        "    # Estimate the causal effect\n",
        "    causal_estimate = causal_model.estimate_effect(identified_estimand)\n",
        "    return causal_estimate"
      ],
      "metadata": {
        "id": "eWC87kJwjpzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Performance Evaluation\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Using accuracy and F1-score for evaluation\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "    return accuracy, f1"
      ],
      "metadata": {
        "id": "TQfM5E1bpf7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Main Execution Workflow\n",
        "def main():\n",
        "    # Load Data (placeholder for actual dataset)\n",
        "    # The raw link to the CSV file in the GitHub repository\n",
        "    url = 'https://raw.githubusercontent.com/dzrich/PhD_Research_in_CausalML/main/Data/CreditRiskDataset.csv'\n",
        "\n",
        "    # Load the CSV file into a pandas DataFrame\n",
        "    CreditRiskDataset = pd.read_csv(url)\n",
        "\n",
        "\n",
        "    # Data Engineering\n",
        "    X, y = data_engineering(CreditRiskDataset)\n",
        "\n",
        "    # Split Data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train Correlational ML Model\n",
        "    corr_model = train_corr_ml_model(X_train, y_train)\n",
        "    corr_accuracy, corr_f1 = evaluate_model(corr_model, X_test, y_test)\n",
        "    print(f\"Correlational Model Accuracy: {corr_accuracy}, F1-Score: {corr_f1}\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "1C6lijEAmmrl",
        "outputId": "0745d034-7590-4a0c-9020-5f1700b6e436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-47c5e0d99e4f>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Correlational Model Accuracy: {corr_accuracy}, F1-Score: {corr_f1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-47c5e0d99e4f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Split Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Train Correlational ML Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2649\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2305\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2306\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.head(5))\n"
      ],
      "metadata": {
        "id": "MiO7P7YG8vEQ",
        "outputId": "3d8d449a-ec2b-4f85-fdcf-f394d5ff7d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-efcd26b22433>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Train Causal ML Model (Placeholder - actual implementation depends on data specifics)\n",
        "    causal_estimate = train_causal_ml_model(X_train, y_train, CreditRiskDataset)\n",
        "    print(f\"Causal Estimate: {causal_estimate.value}\")\n",
        "\n",
        "    # Generalizability Check (Example Decision Logic)\n",
        "    if corr_accuracy > 0.8:  # Threshold for generalizability - adjust based on context\n",
        "        print(\"Correlational ML Model is generalizable. Deploying...\")\n",
        "    elif causal_estimate.value > 0.1:  # Example threshold for causal model\n",
        "        print(\"Causal ML Model is generalizable. Deploying...\")\n",
        "    else:\n",
        "        print(\"Models are not generalizable. Reiterate data engineering or model training steps.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "cEZS5iDa8lWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/dzrich/PhD_Research_in_CausalML/tree/main/Data"
      ],
      "metadata": {
        "id": "rluCHN5oAdCa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMd8JcuzioSk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}